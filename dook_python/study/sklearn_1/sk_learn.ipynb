{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "DESC",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-1-a92f083054f3\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\u0027hello3\u0027\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Users/chenyahui/dook_python/venv/lib/python2.7/site-packages/sklearn/utils/__init__.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 104\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: DESC"
          ],
          "output_type": "error"
        }
      ],
      "source": "# 鸢尾花 模型\n# -*- coding: UTF-8 -*-   \nfrom aetypes import end\nfrom sklearn import datasets\n\niris \u003d datasets.load_iris() # 导入数据集\nx\u003diris.data\ny\u003diris.target\nprint(iris.DESC)\nprint \u0027hello3\u0027\n\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0[-0.6600737  -0.0558978   0.82286793  1.1003977  -0.93493796]\n1[ 0.4113583   0.06249216 -0.90760075 -1.41296696  2.059838  ]\n1[ 1.52452016 -0.01867812  0.20900899  1.34422289 -1.61299022]\n0[-1.25725859  0.02347952 -0.28764782 -1.32091378 -0.88549315]\n0[-3.28323172  0.03899168 -0.43251277 -2.86249859 -1.10457948]\n1[ 1.68841011  0.06754955 -1.02805579 -0.83132182  0.93286635]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# 创建数据集\n# from __future__ import print_function\nfrom sklearn.datasets.samples_generator import make_classification\n\nX, y \u003d make_classification(n_samples\u003d6, n_features\u003d5, n_informative\u003d2, \n    n_redundant\u003d2, n_classes\u003d2, n_clusters_per_class\u003d2, scale\u003d1.0, \n    random_state\u003d20)\n# n_samples：指定样本数\n# n_features：指定特征数\n# n_classes：指定几分类\n# random_state：随机种子，使得随机状可重\nfor x_,y_ in zip(X,y):\n    print(y_)\n    print(x_)\n\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[(1, 4), (2, 5)]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "a\u003d[1,2,3]\nb\u003d[4,5]\nc\u003d[6,7,8,9]\nl1\u003dzip(a,b)\nl2\u003dzip(a,c)\nl3\u003dzip(c,a)\nprint (l1)\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import sys\nprint(\"Python version: {}\". format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\". format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\". format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\". format(sp.__version__))\nimport IPython\nfrom IPython import display\nprint(\"IPython version: {}\". format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\u0027ignore\u0027)\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#可视化\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n#from pandas.tools.plotting import scatter_matrix\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[1.  1.1]\n [1.  1. ]\n [0.  0. ]\n [0.  0.1]]\nhello\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# -*- coding: UTF-8 -*-\nfrom numpy import *\nimport operator\n\ndef createDataSet():\n    #创建4行2列二维数组。\n    group \u003darray([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])\n    lables\u003d[\u0027A\u0027,\u0027A\u0027,\u0027B\u0027,\u0027B\u0027]\n    print group\n    return group,lables\n\ndef classify0(inX,dataSet,labels,k):\n    dataSetSize \u003d dataSet.shape[0]\n    #在列方向上重复dataSetSize 次，行1次\n    diffMat\u003dtile(inX,(dataSetSize,1))-dataSet\n    # **2 平方\n    sqdiffMat\u003ddiffMat**2\n    #注意是axis\u003d1，也就是array每个二维数组成员进行求和(行求和)，如果是axis\u003d0就是列求和\n    sqDistances\u003dsqdiffMat.sum(axis\u003d1)\n    distances\u003dsqDistances**0.5\n    #argsort函数返回的是数组值从小到大的索引值\n    sortedDistIndicies \u003d distances.argsort()\n    #classCount定义为存储字典，里面有‘A’和‘B’\n    classCount\u003d{}\n    for i in range(k):\n        temp1\u003dsortedDistIndicies[i]\n        voteIlabel \u003d labels[temp1]\n        classCount[voteIlabel] \u003d classCount.get(voteIlabel, 0) + 1\n    #sorted函数的第二个参数导入了运算符模块的itemgetter方法，按照第二个元素的次序（即数字）进行排序，由于此处reverse\u003dTrue，是逆序，所以按照从大到小的次序排列。\n    sortedClassCount \u003d sorted(classCount.iteritems(), key\u003doperator.itemgetter(1), reverse\u003dTrue)\n    return sortedClassCount[0][0]\ndataset,group\u003dcreateDataSet()\nary\u003dclassify0([1,0.5],dataset,group,4)\nprint \u0027hello\u0027\n\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "hello\n",
            "You will probably like this person in large doses\nhello\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# -*- coding: UTF-8 -*-\n# -*- coding: UTF-8 -*-\n\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator\n\ndef createDataSet():\n    #创建4行2列二维数组。\n    group \u003darray([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])\n    lables\u003d[\u0027A\u0027,\u0027A\u0027,\u0027B\u0027,\u0027B\u0027]\n    print group\n    return group,lables\n\ndef file2matrix(file):\n    fr\u003dopen(file)\n    number\u003dlen(fr.readlines())\n    returnMat\u003dzeros((number,3))\n    classLabelVector\u003d[]\n    fr\u003dopen(file)\n    index\u003d0\n    for line in fr.readlines():\n        line\u003dline.strip()\n        listFromLine\u003dline.split(\u0027\\t\u0027)\n        returnMat[index,:]\u003dlistFromLine[0:3]\n        classLabelVector.append(int(listFromLine[-1]))\n        index+\u003d1\n    return  returnMat,classLabelVector\n\n\ndef demo():\n    returnMat \u003d zeros((100, 3))\n    classLabelVector \u003d []\n    line\u003d\u00271 2 3\u0027.strip()\n    listFromLine\u003dline.split(\u0027 \u0027)\n    returnMat[0,:]\u003dlistFromLine[0:3]\n    classLabelVector.append(int(listFromLine[-1]))\n    return \u0027null\u0027;\n\n\ndef autoNorm(dataset):\n    min \u003ddataset.min(0)\n    max\u003ddataset.max(0)\n    range\u003dmax-min\n    normdataset\u003dzeros(shape(dataset))\n    m\u003ddataset.shape[0]\n    normdataset\u003ddataset-tile(min,(m,1))\n    normdataset\u003dnormdataset/tile(range,(m,1))\n    return normdataset,range,min\n\ndef classify0(inX, dataSet, labels, k):\n    dataSetSize \u003d dataSet.shape[0]\n    diffMat \u003d tile(inX, (dataSetSize,1)) - dataSet\n    sqDiffMat \u003d diffMat**2\n    sqDistances \u003d sqDiffMat.sum(axis\u003d1)\n    distances \u003d sqDistances**0.5\n    sortedDistIndicies \u003d distances.argsort()\n    classCount\u003d{}\n    for i in range(k):\n        voteIlabel \u003d labels[sortedDistIndicies[i]]\n        classCount[voteIlabel] \u003d classCount.get(voteIlabel,0) + 1\n    sortedClassCount \u003d sorted(classCount.iteritems(), key\u003doperator.itemgetter(1), reverse\u003dTrue)\n    return sortedClassCount[0][0]\n\ndef datingClassTest():\n    hoRatio \u003d 0.50\n    file\u003d\u0027/Users/chenyahui/Documents/dook_analysis/knn/datingTestSet2.txt\u0027\n    datingDataMat,datingLabels \u003d file2matrix(file)       #load data setfrom file\n    normMat, ranges, minVals \u003d autoNorm(datingDataMat)\n    m \u003d normMat.shape[0]\n    numTestVecs \u003d int(m*hoRatio)\n    errorCount \u003d 0.0\n    for i in range(numTestVecs):\n        classifierResult \u003d classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)\n        print \"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, datingLabels[i])\n        if (classifierResult !\u003d datingLabels[i]): errorCount +\u003d 1.0\n    print \"the total error rate is: %f\" % (errorCount/float(numTestVecs))\n    print errorCount\n\n\ndef classifyPerson():\n    file\u003d\u0027/Users/chenyahui/Documents/dook_analysis/knn/datingTestSet2.txt\u0027\n    resultList \u003d [\u0027not at all\u0027, \u0027in small doses\u0027, \u0027in large doses\u0027]\n    percentTats \u003d float(raw_input(\"percentage of time spent playing video games?\"))\n    ffMiles \u003d float(raw_input(\"frequent flier miles earned per year?\"))\n    iceCream \u003d float(raw_input(\"liters of ice cream consumed per year?\"))\n    datingDataMat, datingLabels \u003d file2matrix(file)\n    normMat, ranges, minVals \u003d autoNorm(datingDataMat)\n    inArr \u003d array([ffMiles, percentTats, iceCream])\n    classifierResult \u003d classify0((inArr - minVals) / ranges, normMat, datingLabels, 3)\n    print \"You will probably like this person\", resultList[classifierResult - 1]\nprint \u0027hello\u0027 ;\n#demo()\n#datingDataMat,datingLabels\u003dfile2matrix(file\u003d\u0027/Users/chenyahui/Documents/dook_analysis/knn/datingTestSet2.txt\u0027);\n\n\n# normdataset,range,min \u003dautoNorm(datingDataMat)\n# fig\u003dplt.figure()\n# ax\u003dfig.add_subplot(111)\n# a1\u003ddatingDataMat[:,0]\n# a2\u003ddatingDataMat[:,1]\n# a3\u003d15.0*array(datingLabels)\n# a4\u003d15.0*array(datingLabels)\n# ax.scatter(datingDataMat[:,0], datingDataMat[:,1], 15.0*array(datingLabels), 15.0*array(datingLabels))  #scatter函数是用来画散点图的\n# plt.show()\n# group , labels \u003dcreateDataSet()\n# s\u003dclassify0([0,0], group, labels, 3)\n# print s\n\n# datingClassTest()\nclassifyPerson()\nprint \u0027hello\u0027 ;\n\nzer \u003d sklearn.preprocessing.Normalizer()\n# zer.fit(train_data)\n# \n# data2 \u003d zer.transform(train_data)\n# data2 \u003d zer.fit_transform(train_data)\n# # do train\n# \n# test_data \u003d zer.transform(test_data)\n\n# do test\n\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% 人物魅力分析\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# -*- coding: UTF-8 -*-\n\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator\nfrom os import listdir\n\n\n\n# 首先我们是将宽高是32X32的像素的黑白图像转换成文本文件存储，但我们知道文本文件必须转换成特征向量，才能进入k-近邻算法中进行处理，\ndef img2vector(filename):\n    returnVect \u003d zeros((1,1024))\n    fr \u003d open(filename)\n    for i in range(32):\n        lineStr \u003d fr.readline()\n        for j in range(32):\n            returnVect[0,32*i+j] \u003d int(lineStr[j])\n    return returnVect\n\ndef handwritingClassTest():\n    hwLabels \u003d []\n    trainingFileList \u003d listdir(\u0027/Users/chenyahui/Documents/dook_analysis/knn/trainingDigits\u0027)           #load the training set\n    m \u003d len(trainingFileList)\n    trainingMat \u003d zeros((m,1024))\n    for i in range(m):\n        fileNameStr \u003d trainingFileList[i]\n        fileStr \u003d fileNameStr.split(\u0027.\u0027)[0]     #take off .txt\n        classNumStr \u003d int(fileStr.split(\u0027_\u0027)[0])\n        hwLabels.append(classNumStr)\n        trainingMat[i,:] \u003d img2vector(\u0027/Users/chenyahui/Documents/dook_analysis/knn/trainingDigits/%s\u0027 % fileNameStr)\n    testFileList \u003d listdir(\u0027/Users/chenyahui/Documents/dook_analysis/knn/testDigits\u0027)        #iterate through the test set\n    errorCount \u003d 0.0\n    mTest \u003d len(testFileList)\n    for i in range(mTest):\n        fileNameStr \u003d testFileList[i]\n        fileStr \u003d fileNameStr.split(\u0027.\u0027)[0]     #take off .txt\n        classNumStr \u003d int(fileStr.split(\u0027_\u0027)[0])\n        vectorUnderTest \u003d img2vector(\u0027/Users/chenyahui/Documents/dook_analysis/knn/testDigits/%s\u0027 % fileNameStr)\n        classifierResult \u003d classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n        print \"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, classNumStr)\n        if (classifierResult !\u003d classNumStr): errorCount +\u003d 1.0\n    print \"\\nthe total number of errors is: %d\" % errorCount\n    print \"\\nthe total error rate is: %f\" % (errorCount/float(mTest))\n\n\n\ndef classify0(inX, dataSet, labels, k):\n    dataSetSize \u003d dataSet.shape[0]\n    diffMat \u003d tile(inX, (dataSetSize,1)) - dataSet\n    sqDiffMat \u003d diffMat**2\n    sqDistances \u003d sqDiffMat.sum(axis\u003d1)\n    distances \u003d sqDistances**0.5\n    sortedDistIndicies \u003d distances.argsort()\n    classCount\u003d{}\n    for i in range(k):\n        voteIlabel \u003d labels[sortedDistIndicies[i]]\n        classCount[voteIlabel] \u003d classCount.get(voteIlabel,0) + 1\n    sortedClassCount \u003d sorted(classCount.iteritems(), key\u003doperator.itemgetter(1), reverse\u003dTrue)\n    return sortedClassCount[0][0]\n\nprint \u0027start\u0027 ;\nhandwritingClassTest()\nprint \u0027end\u0027 ;\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% 数字图片识别\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# -*- coding: UTF-8 -*-\n\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator\nfrom os import listdir\nfrom math import log\nfrom datetime import datetime as dt\nimport numpy as np\nimport math\n\ndef calcShannonEnt(dataSet):\n    numEntries \u003d len(dataSet)\n    labelCounts \u003d {}\n    for featVec in dataSet: #the the number of unique elements and their occurance\n        currentLabel \u003d featVec[-1]\n        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] \u003d 0\n        labelCounts[currentLabel] +\u003d 1\n    shannonEnt \u003d 0.0\n    for key in labelCounts:\n        prob \u003d float(labelCounts[key])/numEntries\n        shannonEnt -\u003d prob * log(prob,2)     #log base 2\n        print dataSet,shannonEnt\n    return shannonEnt\n\ndef createDataSet():\n    dataSet \u003d [[1, 1, \u0027yes\u0027],\n               [1, 1, \u0027yes\u0027],\n               [1, 0, \u0027no\u0027],\n               [0, 1, \u0027no\u0027],\n               [0, 1, \u0027no\u0027]]\n    labels \u003d [\u0027no surfacing\u0027,\u0027flippers\u0027]\n    #change to discrete values\n    return dataSet, labels\n\n\ndef splitDataSet(dataSet, axis, value):\n    retDataSet \u003d []\n    for featVec in dataSet:\n        if featVec[axis] \u003d\u003d value:\n            reducedFeatVec \u003d featVec[:axis]     #chop out axis used for splitting\n            reducedFeatVec.extend(featVec[axis+1:])\n            retDataSet.append(reducedFeatVec)\n    return retDataSet\n\n# 找出最佳分类特征\ndef chooseBestFeatureToSplit(dataSet):\n    numFeatures \u003d len(dataSet[0]) - 1      #the last column is used for the labels\n    baseEntropy \u003d calcShannonEnt(dataSet) #calculate the original entropy\n    bestInfoGain \u003d 0.0;\n    bestFeature \u003d -1\n    for i in range(numFeatures):        #iterate over all the features\n        featList \u003d [example[i] for example in dataSet]#create a list of all the examples of this feature\n        uniqueVals \u003d set(featList)       #get a set of unique values\n        newEntropy \u003d 0.0\n        for value in uniqueVals:\n            subDataSet \u003d splitDataSet(dataSet, i, value)\n            prob \u003d len(subDataSet)/float(len(dataSet))\n            newEntropy +\u003d prob * calcShannonEnt(subDataSet)\n        infoGain \u003d baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy\n        if (infoGain \u003e bestInfoGain):       #compare this to the best gain so far\n            bestInfoGain \u003d infoGain         #if better than current best, set to best\n            bestFeature \u003d i\n    return bestFeature                      #returns an integer\n\n\n\n\ndef createTree(dataSet,labels):\n    classList \u003d [example[-1] for example in dataSet]\n    if classList.count(classList[0]) \u003d\u003d len(classList):\n        return classList[0]#stop splitting when all of the classes are equal\n    if len(dataSet[0]) \u003d\u003d 1: #stop splitting when there are no more features in dataSet\n        return majorityCnt(classList)\n    bestFeat \u003d chooseBestFeatureToSplit(dataSet)\n    bestFeatLabel \u003d labels[bestFeat]\n    myTree \u003d {bestFeatLabel:{}}\n    del(labels[bestFeat])              #delete the best feature , so it can find the next best feature\n    featValues \u003d [example[bestFeat] for example in dataSet]\n    uniqueVals \u003d set(featValues)\n    for value in uniqueVals:\n        subLabels \u003d labels[:]       #copy all of labels, so trees don\u0027t mess up existing labels\n        myTree[bestFeatLabel][value] \u003d createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n    return myTree\n\n\n\ndef classify(inputTree,featLabels,testVec):\n    firstStr \u003d inputTree.keys()[0]\n    secondDict \u003d inputTree[firstStr]\n    featIndex \u003d featLabels.index(firstStr)\n    key \u003d testVec[featIndex]\n    valueOfFeat \u003d secondDict[key]\n    if isinstance(valueOfFeat, dict):\n        classLabel \u003d classify(valueOfFeat, featLabels, testVec)\n    else: classLabel \u003d valueOfFeat\n    return classLabel\n\n\n\n\ndef majorityCnt(classList):\n    classCount\u003d{}\n    for vote in classList:\n        if vote not in classCount.keys(): classCount[vote] \u003d 0\n        classCount[vote] +\u003d 1\n    sortedClassCount \u003d sorted(classCount.iteritems(), key\u003doperator.itemgetter(1), reverse\u003dTrue)\n    return sortedClassCount[0][0]\n\n\ndef storeTree(inputTree, filename):\n    import pickle\n    fw \u003d open(filename, \u0027w\u0027)\n    pickle.dump(inputTree, fw)\n    fw.close()\n\n\ndef grabTree(filename):\n    import pickle\n    fr \u003d open(filename)\n    return pickle.load(fr)\n\n\n\n\n\nprint \u0027start\u0027,dt.now()\nmyDat, labels \u003d createDataSet()\n# shannonEnt\u003dcalcShannonEnt(myDat)\n\n# retDataSet\u003dsplitDataSet(myDat,0,1)\n#\n\n# s4\u003dclassify(myTree,labels,[1,0])\n# s5\u003dclassify(myTree,labels,[1,1])\n\n# chooseBestFeatureToSplit(myDat)\n# classList \u003d np.array(myDat).T[-1]\n\n# s\u003dmajorityCnt(classList)\n\nmyTree\u003dcreateTree(myDat,labels\u003dlabels);\n# myDat, labels \u003d createDataSet()\n# classLabel\u003dclassLabel\u003dclassify(myTree,labels,[1,0])\n#\n# storeTree(myTree,\u0027classifierStorage.txt\u0027)\n# grabTree(\u0027classifierStorage.txt\u0027)\n\n# print s\nprint \u0027end\u0027 ,dt.now()\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% 决策树\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# -*- coding: UTF-8 -*-\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator\nfrom os import listdir\nfrom math import log\nfrom datetime import datetime as dt\nimport numpy as np\nimport math\n\n\ndef loadDataSet():\n    postingList \u003d [[\u0027my\u0027, \u0027dog\u0027, \u0027has\u0027, \u0027flea\u0027, \u0027problems\u0027, \u0027help\u0027, \u0027please\u0027],\n                   [\u0027maybe\u0027, \u0027not\u0027, \u0027take\u0027, \u0027him\u0027, \u0027to\u0027, \u0027dog\u0027, \u0027park\u0027, \u0027stupid\u0027],\n                   [\u0027my\u0027, \u0027dalmation\u0027, \u0027is\u0027, \u0027so\u0027, \u0027cute\u0027, \u0027I\u0027, \u0027love\u0027, \u0027him\u0027],\n                   [\u0027stop\u0027, \u0027posting\u0027, \u0027stupid\u0027, \u0027worthless\u0027, \u0027garbage\u0027],\n                   [\u0027mr\u0027, \u0027licks\u0027, \u0027ate\u0027, \u0027my\u0027, \u0027steak\u0027, \u0027how\u0027, \u0027to\u0027, \u0027stop\u0027, \u0027him\u0027],\n                   [\u0027quit\u0027, \u0027buying\u0027, \u0027worthless\u0027, \u0027dog\u0027, \u0027food\u0027, \u0027stupid\u0027]]\n    classVec \u003d [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not\n    return postingList, classVec\n\n\ndef createVocabList(dataSet):\n    vocabSet \u003d set([])  # create empty set\n    for document in dataSet:\n        vocabSet \u003d vocabSet | set(document)  # union of the two sets\n    return list(vocabSet)\n\n\ndef setOfWords2Vec(vocabList, inputSet):\n    returnVec \u003d [0] * len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] \u003d 1\n        else:\n            print \"the word: %s is not in my Vocabulary!\" % word\n    return returnVec\n\n\ndef trainNB0(trainMatrix,trainCategory):\n    numTrainDocs \u003d len(trainMatrix)\n    numWords \u003d len(trainMatrix[0])\n    pAbusive \u003d sum(trainCategory)/float(numTrainDocs)\n    p0Num \u003d zeros(numWords); p1Num \u003d zeros(numWords)\n    p0Denom \u003d 0.0; p1Denom \u003d 0.0\n    for i in range(numTrainDocs):\n        if trainCategory[i] \u003d\u003d 1:\n            p1Num +\u003d trainMatrix[i]\n            p1Denom +\u003d sum(trainMatrix[i])\n        else:\n            p0Num +\u003d trainMatrix[i]\n            p0Denom +\u003d sum(trainMatrix[i])\n    p1Vect \u003d p1Num/p1Denom\n    p0Vect \u003d p0Num/p0Denom\n    return p0Vect,p1Vect,pAbusive\n\n\n\nprint \u0027start\u0027 ,dt.now()\nlistOPosts,listClasses \u003dloadDataSet()\nmyVocabList \u003d createVocabList(listOPosts)\nreturnVec\u003dsetOfWords2Vec(myVocabList,listOPosts[0])\nreturnVec2\u003dsetOfWords2Vec(myVocabList,listOPosts[3])\ntrainNB0()\n\nprint \u0027end\u0027 ,dt.now()\n\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% 贝叶斯函数\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# -*- coding: UTF-8 -*-\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator\nfrom os import listdir\nfrom math import log\nfrom datetime import datetime as dt\nimport numpy as np\nimport math\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadDataSet():\n    dataMat \u003d []; labelMat \u003d []\n    fr \u003d open(\u0027/Users/chenyahui/Documents/dook_analysis/Ch05/testSet.txt\u0027)\n    for line in fr.readlines():\n        lineArr \u003d line.strip().split()\n        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(int(lineArr[2]))\n    return dataMat,labelMat\n\ndef sigmoid(inX):\n    return 1.0/(1+exp(-inX))\n\ndef gradAscent(dataMatIn, classLabels):\n    dataMatrix \u003d mat(dataMatIn)             #convert to NumPy matrix\n    labelMat \u003d mat(classLabels).transpose() #convert to NumPy matrix\n    m,n \u003d shape(dataMatrix)\n    alpha \u003d 0.001\n    maxCycles \u003d 500\n    weights \u003d ones((n,1))\n    for k in range(maxCycles):              #heavy on matrix operations\n        h \u003d sigmoid(dataMatrix*weights)     #matrix mult\n        error \u003d (labelMat - h)              #vector subtraction\n        weights \u003d weights + alpha * dataMatrix.transpose()* error #matrix mult\n    return weights\n\n\ndef plotdemo():\n    x \u003d np.linspace(-5, 5, 200)\n    y \u003d 1. / (1 + np.exp(-x))\n    plt.figure()\n    plt.plot(x, y)\n    plt.xlabel(\u0027x\u0027)\n    plt.ylabel(\u0027sigmiod(x)\u0027)\n    plt.show()\n\n\ndef plotBestFit(weights):\n    import matplotlib.pyplot as plt\n    dataMat,labelMat\u003dloadDataSet()\n    dataArr \u003d array(dataMat)\n    n \u003d shape(dataArr)[0]\n    xcord1 \u003d []; ycord1 \u003d []\n    xcord2 \u003d []; ycord2 \u003d []\n    for i in range(n):\n        if int(labelMat[i])\u003d\u003d 1:\n            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n        else:\n            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n    fig \u003d plt.figure()\n    ax \u003d fig.add_subplot(111)\n    ax.scatter(xcord1, ycord1, s\u003d30, c\u003d\u0027red\u0027, marker\u003d\u0027s\u0027)\n    ax.scatter(xcord2, ycord2, s\u003d30, c\u003d\u0027green\u0027)\n    x \u003d arange(-3.0, 3.0, 0.1)\n    y \u003d (-weights[0]-weights[1]*x)/weights[2]\n    ax.plot(x, y)\n    plt.xlabel(\u0027X1\u0027); plt.ylabel(\u0027X2\u0027);\n    plt.show()\n\n\ndef stocGradAscent0(dataMatrix, classLabels):\n    m,n \u003d shape(dataMatrix)\n    alpha \u003d 0.01\n    weights \u003d ones(n)   #initialize to all ones\n    for i in range(m):\n        h \u003d sigmoid(sum(dataMatrix[i]*weights))\n        error \u003d classLabels[i] - h\n        weights \u003d weights + alpha * error * dataMatrix[i]\n    return weights\n\n\n\ndef stocGradAscent1(dataMatrix, classLabels, numIter\u003d150):\n    m,n \u003d shape(dataMatrix)\n    weights \u003d ones(n)   #initialize to all ones\n    for j in range(numIter):\n        dataIndex \u003d range(m)\n        for i in range(m):\n            alpha \u003d 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not\n            randIndex \u003d int(random.uniform(0,len(dataIndex)))\n            h \u003d sigmoid(sum(dataMatrix[randIndex]*weights))\n            error \u003d classLabels[randIndex] - h\n            weights \u003d weights + alpha * error * dataMatrix[randIndex]\n            del(dataIndex[randIndex])\n    return weights\n\nprint \u0027start\u0027 ,dt.now()\ndataMat , labelMat \u003d loadDataSet()\nweights\u003dgradAscent(dataMat,labelMat)\n\n# weights\u003dstocGradAscent0(array(dataMat),labelMat)\n# weights\u003dstocGradAscent1(array(dataMat),labelMat)\nprint weights\nplotBestFit(weights)\nprint \u0027end\u0027 ,dt.now()\n\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% 梯度上升下降\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# -*- coding: UTF-8 -*-\nfrom numpy import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator\nfrom os import listdir\nfrom math import log\nfrom datetime import datetime as dt\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef loss(w, b, x, y):\n    diff \u003d w * x + b - y\n    loss \u003d math.pow(diff, 2) * 0.5\n    return loss\n\n\ndef predict(x,w,b):\n    yhat \u003d w * x + b\n    # print \u0027predict w \u0027,w,\u0027 b \u0027,b,\u0027 yhat \u0027,yhat\n    return yhat\n\n\ndef charn(x, y, w, b, lr):\n    i\u003d0\n    while (loss(w, b, x, y) \u003e 0.0001 and (i\u003c100)):\n        yhat \u003d predict(x,w,b)\n        w \u003d w - lr * (yhat - y) * x\n        b \u003d b - lr * (yhat - y)\n        i\u003di+1\n        print \"y \", y, \" yhat \", \" w \", w, \" b \", b, \" loss \", loss(w, b, x, y)\n\n\ndef computer_loss(w,b,x_ary,y_ary):\n    loss_all\u003d0\n    for i in range(0, len(x_ary)):\n        loss_one\u003dloss(w,b, x_ary[i],y_ary[i])\n        loss_all\u003dloss_all+loss_one\n    average\u003dloss_all/len(x_ary)\n    return average\n\ndef charn_all(x_ary, y_ary, w, b, lr):\n    n\u003d0\n    loss\u003dcomputer_loss(w, b, x_ary, y_ary)\n    while (loss\u003e 0.0001 and (n\u003c10000)):\n        wder\u003d0\n        bder\u003d0\n        for i in range(0, len(x_ary)):\n            yhat \u003d predict(x_ary[i], w, b)\n            w \u003d w - lr * (yhat - y_ary[i]) * x_ary[i]\n            b \u003d b - lr * (yhat - y_ary[i])\n            wder\u003dwder+w\n            bder\u003dbder+b\n        w\u003dwder/len(x_ary)\n        b\u003dbder/len(x_ary)\n        n\u003dn+1\n        print \"n \u003d\",n, \" w \u003d \", w, \" b \u003d \", b, \" computer_loss \u003d \", computer_loss(w, b, x_ary, y_ary )\n        return w,b,loss\n\n\ndef plot3d(x_ary, y_ary, w, b, lr):\n   w \u003d np.linspace(-6 * np.pi, 6 * np.pi, 1000)\n   loss\u003dcomputer_loss(w,b,x_ary\u003dx_ary,y_ary\u003dy_ary)\n   fig \u003d plt.figure()\n   ax\u003dAxes3D(fig)\n   ax.plot(w,b, loss)\n   # 显示图\n   plt.show()\n\n\n\ndef plot3d_point():\n    x \u003d np.random.normal(0, 1, 100)\n    y \u003d np.random.normal(0, 1, 100)\n    z \u003d np.random.normal(0, 1, 100)\n    fig \u003d plt.figure()\n    ax \u003d Axes3D(fig)\n    ax.scatter(x, y, z)\n    plt.show()\n\n\n\ndef plot3d_line():\n    # 生成数据\n    x \u003d np.linspace(-6 * np.pi, 6 * np.pi, 1000)\n    y \u003d np.sin(x)\n    z \u003d np.cos(x)\n    # 创建 3D 图形对象\n    fig \u003d plt.figure()\n    ax \u003d Axes3D(fig)\n    # 绘制线型图\n    ax.plot(x, y, z)\n    # 显示图\n    plt.show()\n\n\ndef plot3d_face():\n    # 创建 3D 图形对象\n    fig \u003d plt.figure()\n    ax \u003d Axes3D(fig)\n    # 生成数据\n    X \u003d np.arange(-2, 2, 0.1)\n    Y \u003d np.arange(-2, 2, 0.1)\n    X, Y \u003d np.meshgrid(X, Y)\n    Z \u003d np.sqrt(X ** 2 + Y ** 2)\n\n    # 绘制曲面图，并使用 cmap 着色\n    ax.plot_surface(X, Y, Z, cmap\u003dplt.cm.winter)\n    plt.show()\n\n\ndef plot3d_face_():\n    # 创建 3D 图形对象\n    fig \u003d plt.figure()\n    ax \u003d Axes3D(fig)\n    # 生成数据\n    X \u003d np.arange(0, 10, 0.001)\n    Y \u003d np.arange(0, 10, 0.001)\n    X, Y \u003d np.meshgrid(X, Y)\n    Z \u003d np.sqrt(X ** 2 + Y ** 2)\n\n    # 绘制曲面图，并使用 cmap 着色\n    ax.plot_surface(X, Y, Z, cmap\u003dplt.cm.winter)\n\n\n    plt.show()\n\nprint \u0027start\u0027, dt.now()\narea\u003d[1.03,1.118,1.17,1.19,1.25]\nmoney\u003d[8.6,9.25,9.7,9.88,10.35]\n\n\nw\u003d7.97027544957\nb\u003d0.375681025663\n# s1 \u003d loss(1, 0, 0.85, 7.05)\n# s2 \u003d predict(1.03,w,b)\n# charn(0.85, 7.05,1,0, 0.1)\n\n# s3\u003dcomputer_loss(1,0,area,money)\n# print s3\n\n# s4\u003dcharn_all(x_ary\u003darea,y_ary\u003dmoney,w\u003d1,b\u003d0,lr\u003d0.2)\n\n# print s4\n# plot3d(x_ary\u003darea,y_ary\u003dmoney,w\u003d1,b\u003d0,lr\u003d0.2)\nplot3d_face()\nprint \u0027end\u0027, dt.now()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%  房屋价格模型\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# -*- coding: UTF-8 -*-\nimport numpy as np\n\n\"\"\"\n    这个指南的目的是在一个实际任务上探索scikit-learn的主要工具，在二十个不同的主题上分析一个文本集合。\n    在这一节中，可以看到：\n        1、加载文本文件和类别\n        2、适合机器学习的特征向量提取\n        3、训练线性模型进行分类\n        4、使用网格搜索策略，找到一个很好的配置的特征提取组件和分类器\n\"\"\"\n\n\"\"\"\n    1、Loading the 20 newsgroups dataset 加载20个新闻组数据集\n    为了获得更快的执行时间为第一个例子，我们将工作在部分数据集只有4个类别的数据集中：\n\"\"\"\ncategories \u003d [\u0027alt.atheism\u0027, \u0027soc.religion.christian\u0027, \u0027comp.graphics\u0027, \u0027sci.med\u0027]\nfrom sklearn.datasets import fetch_20newsgroups\n\ntwenty_train \u003d fetch_20newsgroups(subset\u003d\u0027train\u0027, categories\u003dcategories, shuffle\u003dTrue, random_state\u003d42)\nprint(twenty_train.target)\nprint(twenty_train.target_names)  # 训练集中类别的名字，这里只有四个类别\nprint(len(twenty_train.data))  # 训练集中数据的长度\nprint(len(twenty_train.filenames))  # 训练集文件名长度\nprint(\u0027-----\u0027)\nprint(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\nprint(\u0027-----\u0027)\nprint(twenty_train.target_names[twenty_train.target[0]])\nprint(\u0027-----\u0027)\nprint(twenty_train.target[:10])  # 前十个的类别\nprint(\u0027-----\u0027)\nfor t in twenty_train.target[:10]:\n    print(twenty_train.target_names[t])  # 类别的名字\nprint(\u0027-----\u0027)\n\"\"\"\n    2、Extracting features from text files 从文本文件中提取特征\n    为了在文本文件中使用机器学习算法，首先需要将文本内容转换为数值特征向量\n\"\"\"\n\n\"\"\"\n    Bags of words 词袋\n    最直接的方式就是词袋表示法\n        1、为训练集的任何文档中的每个单词分配一个固定的整数ID（例如通过从字典到整型索引建立字典）\n        2、对于每个文档，计算每个词出现的次数，并存储到X[i,j]中。\n\n    词袋表示：n_features 是语料中不同单词的数量，这个数量通常大于100000.\n    如果 n_samples \u003d\u003d 10000，存储X的数组就需要10000*10000*4byte\u003d4GB,这么大的存储在今天的计算机上是不可能实现的。\n    幸运的是，X中的大多数值都是0，基于这种原因，我们说词袋是典型的高维稀疏数据集，我们可以只存储那些非0的特征向量。\n    scipy.sparse 矩阵就是这种数据结构，而scikit-learn内置了这种数据结构。\n\"\"\"\n\n\"\"\"\n    Tokenizing text with scikit-learn 使用scikit-learn标记文本\n    文本处理、分词、过滤停用词都在这些高级组件中，能够建立特征字典并将文档转换成特征向量。\n\"\"\"\nfrom sklearn.feature_extraction.text import CountVectorizer  # sklearn中的文本特征提取组件中，导入特征向量计数函数\n\ncount_vect \u003d CountVectorizer()  # 特征向量计数函数\nX_train_counts \u003d count_vect.fit_transform(twenty_train.data)  # 对文本进行特征向量处理\nprint(X_train_counts)  # 特征向量和特征标签\nprint(X_train_counts.shape)  # 形状\nprint(\u0027-----\u0027)\n\n\"\"\"\n    CountVectorizer支持计算单词或序列的N-grams，一旦合适，这个向量化就可以建立特征词典。\n    在整个训练预料中，词汇中的词汇索引值与其频率有关。\n\"\"\"\nprint(count_vect.vocabulary_.get(u\u0027algorithm\u0027))\nprint(\u0027-----\u0027)\n\n\"\"\"\n    From occurrences to frequencies 从事件到频率\n    计数是一个好的开始，但是也存在一个问题：较长的文本将会比较短的文本有很高的平均计数值，即使他们所表示的话题是一样的。\n    为了避免潜在的差异，它可以将文档中的每个单词出现的次数在文档的总字数的比例：这个新的特征叫做词频：tf\n    tf-idf:词频-逆文档频率\n\"\"\"\nfrom sklearn.feature_extraction.text import TfidfTransformer  # sklearn中的文本特征提取组件中，导入词频统计函数\n\ntf_transformer \u003d TfidfTransformer(use_idf\u003dFalse).fit(X_train_counts)  # 建立词频统计函数,注意这里idf\u003dFalse\nprint(tf_transformer)  # 输出函数属性 TfidfTransformer(norm\u003du\u0027l2\u0027, smooth_idf\u003dTrue, sublinear_tf\u003dFalse, use_idf\u003dFalse)\nprint(\u0027-----\u0027)\nX_train_tf \u003d tf_transformer.transform(X_train_counts)  # 使用函数对文本文档进行tf-idf频率计算\nprint(X_train_tf)\nprint(\u0027-----\u0027)\nprint(X_train_tf.shape)\nprint(\u0027-----\u0027)\n\"\"\"\n    在上面的例子中，使用fit()方法来构建基于数据的预测器，然后使用transform()方法来将计数矩阵用tf-idf表示。\n    这两个步骤可以通过跳过冗余处理，来更快的达到相同的最终结果。\n    这些可以通过使用fit_transform()方法来实现：\n\"\"\"\ntfidf_transformer \u003d TfidfTransformer()  # 这里使用的是tf-idf\nX_train_tfidf \u003d tfidf_transformer.fit_transform(X_train_counts)\nprint(X_train_tfidf)\nprint(X_train_tfidf.shape)\nprint(\u0027-----\u0027)\n\"\"\"\n    Training a classifier 训练一个分类器\n    既然已经有了特征，就可以训练分类器来试图预测一个帖子的类别，先使用贝叶斯分类器，贝叶斯分类器提供了一个良好的基线来完成这个任务。\n    scikit-learn中包括这个分类器的许多变量，最适合进行单词计数的是多项式变量。\n\"\"\"\nfrom sklearn.naive_bayes import MultinomialNB  # 使用sklearn中的贝叶斯分类器，并且加载贝叶斯分类器\n\n# 中的MultinomialNB多项式函数\nclf \u003d MultinomialNB()  # 加载多项式函数\nx_clf \u003d clf.fit(X_train_tfidf, twenty_train.target)  # 构造基于数据的分类器\nprint(x_clf)  # 分类器属性：MultinomialNB(alpha\u003d1.0, class_prior\u003dNone, fit_prior\u003dTrue)\nprint(\u0027-----\u0027)\n\"\"\"\n    为了预测输入的新的文档，我们需要使用与前面相同的特征提取链进行提取特征。\n    不同的是，在转换中，使用transform来代替fit_transform，因为训练集已经构造了分类器\n\"\"\"\ndocs_new \u003d [\u0027God is love\u0027, \u0027OpenGL on the GPU is fast\u0027]  # 文档\nX_new_counts \u003d count_vect.transform(docs_new)  # 构建文档计数\nX_new_tfidf \u003d tfidf_transformer.transform(X_new_counts)  # 构建文档tfidf\npredicted \u003d clf.predict(X_new_tfidf)  # 预测文档\nprint(predicted)  # 预测类别 [3 1]，一个属于3类，一个属于1类\nfor doc, category in zip(docs_new, predicted):\n    print(\u0027%r \u003d\u003e %s\u0027 % (doc, twenty_train.target_names[category]))  # 将文档和类别名字对应起来\nprint(\u0027-----\u0027)\n\"\"\"\n    Building a pipeline 建立管道\n    为了使向量转换更加简单(vectorizer \u003d\u003e transformer \u003d\u003e classifier)，scikit-learn提供了pipeline类来表示为一个复合分类器\n\"\"\"\nfrom sklearn.pipeline import Pipeline\n\ntext_clf \u003d Pipeline([(\u0027vect\u0027, CountVectorizer()), (\u0027tfidf\u0027, TfidfTransformer()), (\u0027clf\u0027, MultinomialNB())])\ntext_clf \u003d text_clf.fit(twenty_train.data, twenty_train.target)\nprint(text_clf)  # 构造分类器，分类器的属性\npredicted \u003d text_clf.predict(docs_new)  # 预测新文档\nprint(predicted)  # 获取预测值\nprint(\u0027-----\u0027)\n\n\"\"\"\n    分析总结：\n        1、加载数据集，主要是加载训练集，用于对数据进行训练\n        2、文本特征提取：\n                对文本进行计数统计 CountVectorizer\n                词频统计  TfidfTransformer  （先计算tf,再计算tfidf）\n        3、训练分类器：\n                贝叶斯多项式训练器 MultinomialNB\n        4、预测文档：\n                通过构造的训练器进行构造分类器，来进行文档的预测\n        5、最简单的方式：\n                通过使用pipeline管道形式，来讲上述所有功能通过管道来一步实现，更加简单的就可以进行预测\n\"\"\"\n\n\"\"\"\n    Evaluation of the performance on the test set 测试集性能评价\n    评估模型的预测精度同样容易：\n\"\"\"\nimport numpy as np\n\ntwenty_test \u003d fetch_20newsgroups(subset\u003d\u0027test\u0027, categories\u003dcategories, shuffle\u003dTrue, random_state\u003d42)\ndocs_test \u003d twenty_test.data\npredicted \u003d text_clf.predict(docs_test)\nprint(np.mean(predicted \u003d\u003d twenty_test.target))  # 预测的值和测试值的比例，mean就是比例函数\nprint(\u0027-----\u0027)  # 精度已经为0.834886817577\n\n\"\"\"\n    精度已经实现了83.4%，那么使用支持向量机(SVM)是否能够做的更好呢，支持向量机(SVM)被广泛认为是最好的文本分类算法之一。\n    尽管，SVM经常比贝叶斯要慢一些。\n    我们可以改变学习方式，使用管道来实现分类：\n\"\"\"\nfrom sklearn.linear_model import SGDClassifier\n\ntext_clf \u003d Pipeline(\n    [(\u0027vect\u0027, CountVectorizer()), (\u0027tfidf\u0027, TfidfTransformer()),\n     (\u0027clf\u0027, SGDClassifier(loss\u003d\u0027hinge\u0027, penalty\u003d\u0027l2\u0027, alpha\u003d1e-3, n_iter\u003d5, random_state\u003d42))])\n# _ \u003d text_clf.fit(twenty_train.data, twenty_train.target)  # 和下面一句的意思一样，一个杠，表示本身\ntext_clf \u003d text_clf.fit(twenty_train.data, twenty_train.target)\npredicted \u003d text_clf.predict(docs_test)\nprint(np.mean(predicted \u003d\u003d twenty_test.target))  # 精度 0.912782956059\nprint(\u0027-----\u0027)\n\"\"\"\n    sklearn进一步提供了结果的更详细的性能分析工具：\n\"\"\"\nfrom sklearn import metrics\n\nprint(metrics.classification_report(twenty_test.target, predicted, target_names\u003dtwenty_test.target_names))\nprint(metrics.confusion_matrix(twenty_test.target, predicted))\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% 文档贝叶斯分类器\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python2",
      "language": "python",
      "display_name": "Python 2"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}